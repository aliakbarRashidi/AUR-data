{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arch Linux Packages (non-AUR Arch Linux packages)\n",
    "\n",
    "In addition to the AUR, we also have core Arch Linux packages that are not included in the AUR. The script below shows how we can scrape data on these packages. I later realized that these packages are available in the [sitemap](https://www.archlinux.org/sitemap-packages.xml). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/brian/Documents/aur/html/package-pages/')\n",
    "driver = webdriver.PhantomJS()\n",
    "base_url = 'https://www.archlinux.org/packages/?page='\n",
    "for i in range(1,101):\n",
    "    search_url = base_url + str(i) + \"&\"\n",
    "    print(search_url)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(2 + np.random.random())\n",
    "    html = driver.page_source.encode('utf-8')\n",
    "    name = \"package_page_\" + str(i)\n",
    "    package_list = open(name+'.txt', 'w+')\n",
    "    package_list.write(str(html))\n",
    "    package_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/brian/Documents/aur/html/package-pages/')\n",
    "files = os.listdir()\n",
    "dict_list = []\n",
    "for file in files:     \n",
    "    f = open(file, 'r')\n",
    "    html = f.read()\n",
    "    b = BeautifulSoup(html, 'lxml')\n",
    "    try: \n",
    "        packages = b.find_all('tr')[1:]\n",
    "        for package in packages:\n",
    "            data = package.find_all('td')\n",
    "            data_dict = {\n",
    "                         \"arch\": data[0].text,\n",
    "                         \"repo\": data[1].text,\n",
    "                         \"name\": data[2].text,\n",
    "                         \"link\": data[2].find('a')['href'],\n",
    "                         \"version\": data[3].text,\n",
    "                         \"description\": data[4].text, \n",
    "                         \"last_updated\": data[5].text,\n",
    "                         \"flag_date\": data[6].text\n",
    "                        }\n",
    "            dict_list.append(data_dict)\n",
    "    except:\n",
    "        print(file)\n",
    "    b.decompose()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['arch', 'repo', 'name', 'link', 'version', 'description', 'last_updated', 'flag_date']\n",
    "df = pd.DataFrame(dict_list, columns=cols)\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('../csv/arch_pack_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Arch Package Details\n",
    "\n",
    "Next we will loop through each of the individual package pages and extract more details about each package. This will be similar to the steps we took with the AUR packages. Instead of saving this file to a csv, we use built in pandas tools for pickling the DataFrame object. The reason for pickling in this case is because some of the DataFrame ojbects are lists. Saving this data type in a csv would convert it to a string and then require is to convert it back to a list when we want to load the data again. Pickling is an easy way to save and load any python object without changing anything about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/brian/Documents/aur/csv/')\n",
    "df = pd.read_csv('../csv/arch_pack_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9983, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/brian/Documents/aur/html/package-details/')\n",
    "package_html_files = os.listdir()\n",
    "base_url = \"https://www.archlinux.org\"\n",
    "for _, package in df.iterrows():\n",
    "    file_name = str(package['name'] + '.txt')\n",
    "    if file_name not in package_html_files: \n",
    "        print(f'Getting: {file_name}')\n",
    "        link = package['link']\n",
    "        html = requests.get(base_url + link).text\n",
    "        time.sleep(2)\n",
    "        f = open(file_name, 'w+')\n",
    "        f.write(str(html))\n",
    "        f.close()\n",
    "        print(f'Finished getting: {file_name}')\n",
    "    else: \n",
    "        print(f'Skip: {_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pkg_dict_list = []\n",
    "\n",
    "err_count = 0\n",
    "err_files = []\n",
    "os.chdir('/home/brian/Documents/aur/html/package-details/')\n",
    "for _, file in enumerate(os.listdir()):\n",
    "    print(_, end=\" \")\n",
    "    if file != 'ghostdriver.log':\n",
    "        f = open(file, 'r')\n",
    "        html = f.read()\n",
    "        b = BeautifulSoup(html, 'lxml')\n",
    "        try:\n",
    "            print(_)\n",
    "            # attributes\n",
    "            pkginfo = b.find('table', attrs={'id':'pkginfo'}).find_all('tr')\n",
    "            \n",
    "            attr_dict = {}\n",
    "            for attribute in pkginfo:\n",
    "                label = attribute.find('th').text.strip(\": \")\n",
    "                value = attribute.find('td').text\n",
    "                \n",
    "                if label == \"Maintainers\":\n",
    "                    if \"Orphan\" in value:\n",
    "                        attr_dict[label] = \"Orphan\"\n",
    "                        primary_maintainer = \"Orphan\"\n",
    "                        num_maintainers = 0\n",
    "                    else:\n",
    "                        values = attribute.find('td').find_all('a')\n",
    "                        values = [i.text for i in values]\n",
    "                        num_maintainers = len(values)\n",
    "                        attr_dict[\"num_maintainers\"] = num_maintainers\n",
    "                        primary_maintainer = values[0]\n",
    "                        attr_dict[\"primary_maintainer\"] = primary_maintainer\n",
    "                        attr_dict[label] = values\n",
    "                \n",
    "                elif label == \"License(s)\":\n",
    "                    values = value.split(\",\")\n",
    "                    license_count = len(values)\n",
    "                    primary_license = values[0]\n",
    "                    attr_dict[\"primary_license\"] = primary_license\n",
    "                    attr_dict[label] = values\n",
    "                else: \n",
    "                    value = value.replace('\\\\n', '\\n')\n",
    "                    value = value.replace('\\\\t', '\\t')\n",
    "                    value = re.sub('[\\t+]', '', value)\n",
    "                    value = re.sub('[\\n+]', '', value)\n",
    "                    attr_dict[label] = value\n",
    "                \n",
    "            # title and version\n",
    "            title = b.find('h2').text\n",
    "            pkg_name = title.split(\" \")[0]\n",
    "            version_number = title.split(\" \")[1]\n",
    "            \n",
    "            attr_dict['package_name'] = pkg_name\n",
    "            attr_dict['version_number'] = version_number\n",
    "\n",
    "            dependencies = []\n",
    "            pkgdeps = b.find('ul', attrs={'id':'pkgdepslist'})\n",
    "            if pkgdeps:\n",
    "                for p in pkgdeps.find_all('li'):\n",
    "                    for link in p.find_all('a'):\n",
    "                        dependencies.append(link.text)\n",
    "\n",
    "            attr_dict['pkgdeps'] = dependencies\n",
    "            \n",
    "            requirements = []\n",
    "            pkgreqs = b.find('ul', attrs={'id':'pkgreqslist'})\n",
    "            if pkgreqs:\n",
    "                for p in pkgreqs.find_all('li'):\n",
    "                    for link in p.find_all('a'):\n",
    "                        requirements.append(link.text)\n",
    "\n",
    "            attr_dict['pkgreqs'] = requirements\n",
    "\n",
    "            pkg_dict_list.append(attr_dict)\n",
    "\n",
    "        except: \n",
    "            err_count += 1\n",
    "            err_files.append(file)\n",
    "            print(\"ERROR\")\n",
    "            print(err_count)\n",
    "            print(file)\n",
    "        b.decompose()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pkg_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('/home/brian/Documents/aur/pickle/core_packages_df.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/home/brian/Documents/aur/pickle/core_packages_df.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"requirements_count\"] = [len(r) for r in df.pkgreqs]\n",
    "df[\"dependencies_count\"] = [len(d) for d in df.pkgdeps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.requirements_count.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
